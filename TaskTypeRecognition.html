<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="Tasks Reflected in the Eyes: Egocentric Gaze-Aware Visual Task Type Recognition in Virtual Reality">
<meta name="author" content="Zhimin Wang">
<title>Tasks Reflected in the Eyes: Egocentric Gaze-Aware Visual Task Type Recognition in Virtual Reality</title>
<!-- Bootstrap core CSS -->
<link href="publication/ismar_journal_2024/pages/css/bootstrap.min.css" rel="stylesheet">
<!-- Custom styles for this template -->
<link href="publication/ismar_journal_2024/pages/css/offcanvas.css" rel="stylesheet">
</head>

	
<body>
<div class="container">
<div class="jumbotron">
<h2>Tasks Reflected in the Eyes: Egocentric Gaze-Aware Visual Task Type Recognition in Virtual Reality</h2>
<p class="abstract">IEEE TVCG (ISMAR Journal-track), 2024.</p>
<p iclass="authors"><a href="https://zhimin-wang.github.io/">Zhimin Wang</a>, and  <a href="https://scholar.google.com/citations?user=9ggbm0QAAAAJ&hl=zh-CN">Feng Lu</a> </p>
<p>
   <a class="btn btn-primary" href="./publication/ismar_journal_2024/pages/pdf/ISMAR2024Journal.pdf">PDF</a>
   <a class="btn btn-primary" href="./publication/ismar_journal_2024/pages/ppt/wang24_ISMAR_journal.pdf">PPT</a>
   <a class="btn btn-primary" href="https://drive.google.com/file/d/1HW-MxPx6v0HxBBAq_IATj5eouSvG3daU/view?usp=sharing">Dataset</a> 
   <a class="btn btn-primary" href="https://github.com/zhimin-wang/TRCLP">Code</a> 
</div>    


<!--teaser image-->	
<img src="publication/ismar_journal_2024/pages/image/teaser.png" style="width:100%; margin-right:-20px; margin-top:-10px;">


<hr>
<div>
<h3>Abstract</h3>
<p>
  With eye tracking finding widespread utility in augmented reality and virtual reality headsets, eye gaze has the potential to recognize users’ visual tasks and adaptively adjust virtual content displays, thereby enhancing the intelligence of these headsets. However, current studies on visual task recognition often focus on scene-specific tasks, like copying tasks for office environments, which lack applicability to new scenarios, e.g., museums. In this paper, we propose four scene-agnostic task types for facilitating task type recognition across a broader range of scenarios. We present a new dataset that includes eye and head movement data recorded from 20 participants while they engaged in four task types across 15 360-degree VR videos. Using this dataset, we propose an egocentric gaze-aware task type recognition method, TRCLP, which achieves promising results. Additionally, we illustrate the practical applications of task type recognition with three examples. Our work offers valuable insights for content developers in designing task-aware intelligent applications. Our dataset and source code will be released upon acceptance.
</p>
</div>
	

<div class="section">
<h3>Presentation Video:</h3>
<object type='application/x-shockwave-flash' style='width:1024px; height:608px;' data='https://www.youtube.com/v/SGZTOGx_d0M'>
<param name='movie' value='https://www.youtube.com/v/SGZTOGx_d0M' />
</object>
</div>

<div class="section">
  <h3>Supplymental Video:</h3>
  <object type='application/x-shockwave-flash' style='width:1024px; height:608px;' data='https://www.youtube.com/v/HQxJMDK0lHE'>
  <param name='movie' value='https://www.youtube.com/v/HQxJMDK0lHE' />
  </object>
  </div>

		
<div class="section">
<h3>Related Work</h3>
<hr>
<p>Our related work:</p>
<p><a href="https://zhimin-wang.github.io/GVC_See_Through_Vision.html">Gaze-Vergence-Controlled See-Through Vision in Augmented Reality</a></p>
<p><a href="https://zhimin-wang.github.io/publication/thms_2021/pages/21_THMS.html">Interaction With Gaze, Gesture, and Speech in a Flexibly Configurable Augmented Reality System</a></p>
<p><a href="https://zhimin-wang.github.io/publication/ismar_2021/pages/21_ISMAR.html">Edge-Guided Near-Eye Image Analysis for Head Mounted Displays</a></p>
</div>

	
<h3>Bibtex</h3>
<hr>
<div class="bibtexsection">
  @ARTICLE{Wang_TVCG2024A,
    author={Wang, Zhimin and Lu, Feng},
    journal={IEEE Transactions on Visualization and Computer Graphics}, 
    title={Tasks Reflected in the Eyes: Egocentric Gaze-Aware Visual Task Type Recognition in Virtual Reality}, 
    year={2024},
    volume={30},
    number={11},
    pages={7277-7287},
    keywords={Visualization;Switches;Extended reality;Measurement;Tracking;Annotations;Gaze tracking;Virtual reality;eye tracking;visual task type recognition;deep learning;intelligent application},
    doi={10.1109/TVCG.2024.3456164}}
  
</div>
		
    
<!-- <hr>
<footer>
<p>Send feedback and questions to <a href="https://cranehzm.github.io/">Zhiming Hu</a>.</p>
<p>Thanks to Vincent Sitzmann for his website template. © 2017</p>
</footer> -->


</div><!--/.container-->
</body></html>